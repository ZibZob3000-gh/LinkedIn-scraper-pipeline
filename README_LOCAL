# ğŸ’» Local Setup for Skill Extractor ETL Pipeline

This guide explains how to run the ETL pipeline **directly from the source code (GitHub)**.

---

## ğŸ§© Requirements

Before running, please make sure you have:

1. **Python 3.10+**
   Install from: [https://www.python.org/downloads/](https://www.python.org/downloads/)

2. **Pip**
   Comes with Python by default. You can upgrade it:
   ```bash
   python -m pip install --upgrade pip
   ```

3. **Ollama**
   Install Ollama (local model runner):
   ğŸ‘‰ [https://ollama.com/download](https://ollama.com/download)

---

## ğŸ§  Step 1: Download the Llama Model

After installing Ollama, download the required model:
```bash
ollama pull llama3:8b-instruct-q4_K_M
```

---

## ğŸ§© Step 2: Start the Ollama Server

Start the Ollama background server:
```bash
ollama serve &
```

Keep this terminal open while you run the pipeline.
This makes the model available at `http://localhost:11434`.

---

## ğŸ§° Step 3: Install Project Dependencies

Navigate to the cloned project folder and install all requirements:
```bash
pip install -r requirements.txt
```

---

## ğŸš€ Step 4: Run the Pipeline

Run the main script:
```bash
python main.py
```

The ETL pipeline will connect to Ollama and start processing data.

---

## ğŸ“ Notes

- The model must be downloaded **locally** for the pipeline to work.
- If you restart your computer, you may need to start the Ollama server again (`ollama serve &`).
- You can verify that the model is available using:
  ```bash
  ollama list
  ```

---

âœ… **Thatâ€™s it!**
You can now run the ETL pipeline directly from your local environment.
